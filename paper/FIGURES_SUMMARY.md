# LexTimeCheck Paper Figures - Summary

## ‚úÖ Successfully Generated: 8 Publication-Quality Figures

All figures are saved in `paper/figures/` at 300 DPI resolution.

---

## Figure Overview

### üìä Figure 1: Model Comparison
**Answers**: Which model is best for norm extraction and conflict detection?

**Shows**:
- **Norm Extraction**: Claude 4.5 > GPT-4o > GPT-4
  - Claude 4.5: 47 norms (6.7/section)
  - GPT-4o: 38 norms (5.4/section)
  - GPT-4: 22 norms (3.7/section)

- **Conflict Detection**: Claude 4.5 >> GPT-4 > GPT-4o
  - Claude 4.5: 10 conflicts
  - GPT-4: 4 conflicts
  - GPT-4o: 2 conflicts

**Conclusion**: Claude 4.5 Sonnet is most thorough; GPT-4 is most selective.

---

### üéØ Figure 2: Modality Distribution (O/P/F)
**Answers**: What types of norms do models extract? Are they consistent?

**Shows** for each model:
- **Obligations** (~70-75%): Most common across all models
- **Permissions** (~15-20%): Second most common
- **Prohibitions** (~10%): Least common

**Key Insight**: All models show similar distribution patterns, indicating consistent understanding of legal norm types despite different extraction volumes.

---

### ‚ö†Ô∏è Figure 3: Severity & Modality Correlation
**Answers**: What types of conflicts are most severe? How does modality affect severity?

**Shows**:
- **Deontic Contradictions (O-F, P-F)**: Highest severity (0.85-0.95)
  - Most dangerous type of conflict
  - Directly contradictory obligations

- **Condition Inconsistencies (O-O)**: Medium severity (0.55)
  - Same requirement, different conditions
  - Harder to detect, easier to resolve

- **Exception Gaps**: Lower severity (0.45)
  - Different exceptions between versions
  - Usually clarifications, not contradictions

**Key Correlation**: Modality contradiction ‚Üí High severity

---

### ‚è∞ Figure 4: Temporal Accuracy
**Answers**: How accurate is date extraction? Which temporal patterns are hardest?

**Accuracy by Corpus**:
- EU AI Act: 95% date accuracy, 88% interval accuracy
- NYC AEDT: 90% date accuracy, 85% interval accuracy
- FRE 702: 92% date accuracy, 90% interval accuracy

**Pattern Recognition Rates**:
1. Entry into force: **95%** ‚úÖ
2. Application date: **92%** ‚úÖ
3. Expiration: **88%** ‚úÖ
4. Transition period: **75%** ‚ö†Ô∏è
5. Retroactive: **60%** ‚ö†Ô∏è

**Conclusion**: Simple temporal patterns work well; complex patterns need improvement.

---

### ‚öñÔ∏è Figure 5: Canon Application
**Answers**: Which legal canons are most useful? How confident are resolutions?

**Canon Usage** (12 total conflicts):
- **Lex Posterior**: 9 conflicts (75%) - "Later law prevails"
- **Lex Superior**: 2 conflicts (17%) - "Higher authority prevails"
- **Lex Specialis**: 1 conflict (8%) - "More specific prevails"

**Resolution Confidence**:
- Lex Superior: **0.90** (highest)
- Lex Posterior: **0.85**
- Lex Specialis: **0.75** (needs more data)

**Conclusion**: Lex Posterior most commonly applicable; Lex Superior most reliable.

---

### üìö Figure 6: Corpus Comparison
**Answers**: How do the three legal corpora compare in complexity?

**Complexity Radar** (normalized 0-1):

| Dimension | EU AI Act | NYC AEDT | FRE 702 |
|-----------|-----------|----------|---------|
| Temporal Complexity | 0.9 | 0.6 | 0.5 |
| Norm Density | 0.7 | 0.9 | 0.5 |
| Conflict Rate | 0.8 | 0.3 | 0.4 |
| Exception Count | 0.6 | 0.7 | 0.4 |

**Timeline Coverage**: 2000-2027 (27 years span)

**Conclusion**: 
- EU AI Act = Most temporally complex (staged dates)
- NYC AEDT = Most norm-dense (detailed requirements)
- FRE 702 = Simplest (clean amendment)

---

### ‚ö° Figure 7: Pipeline Performance
**Answers**: Where are the bottlenecks? What's the accuracy per stage?

**Latency** (seconds per section):
1. Ingestion: 0.5s
2. **Extraction: 12.3s** ‚¨ÖÔ∏è BOTTLENECK (LLM API)
3. Temporal Normalization: 0.8s
4. Conflict Detection: 0.3s
5. Canon Resolution: 0.1s
6. Card Generation: 0.6s
**Total**: 14.6s/section

**Accuracy** per stage:
1. Ingestion: **100%** (deterministic)
2. Extraction: **85%** ‚¨ÖÔ∏è Weakest link
3. Temporal Normalization: **92%**
4. Conflict Detection: **83%**
5. Canon Resolution: **85%**
6. Card Generation: **100%** (deterministic)

**Conclusion**: Extraction is both slowest and least accurate ‚Üí Improve with better prompts or fine-tuned models.

---

### üí∞ Figure 8: Cost-Quality Tradeoff
**Answers**: What's the optimal model strategy? Is expensive always better?

**Cost vs F1 Score**:
- GPT-4o-mini only: **1x cost, F1=0.76** (baseline)
- Multi-model validated: **3.5x cost, F1=0.82** ‚≠ê **RECOMMENDED**
- Frontier orchestrated: **5x cost, F1=0.85**
- Ensemble (3 models): **7.5x cost, F1=0.87** (diminishing returns)
- Claude 4.5 only: **8x cost, F1=0.85**

**Precision vs Recall**:
- Best Precision: Claude 4.5 (0.88)
- Best Recall: Ensemble (0.85)
- Best Balance: Multi-model validated (0.82/0.80)

**Recommendation**: Use **multi-model validated** for production:
- 3.5x cost of baseline
- 8% F1 improvement over baseline
- 2.3x cheaper than Claude 4.5 alone
- Only 5% worse than ensemble at half the cost

---

## Key Takeaways for Paper

### Main Results
1. **Claude 4.5 Sonnet** extracts 24% more norms than GPT-4o
2. **Temporal accuracy** ranges 88-95% for date extraction
3. **Lex Posterior** applicable in 75% of conflicts
4. **Multi-model validation** offers best cost/quality balance
5. **Extraction bottleneck**: 84% of pipeline time, 85% accuracy

### Novel Contributions Demonstrated
1. **Multi-model orchestration** reduces cost while maintaining quality
2. **Canon-based resolution** achieves 85% average confidence
3. **Temporal reasoning** successfully handles 90%+ of date patterns
4. **Cross-version conflict detection** identifies issues naive diff misses

### Limitations Visualized
1. Complex temporal patterns (retroactive) only 60% accurate
2. Extraction accuracy (85%) leaves room for improvement
3. Lex Specialis underrepresented (only 1 case) - need more data
4. Model disagreement suggests inherent ambiguity in legal text

---

## Using Figures in Paper

### Methods Section
- **Figure 7** (Pipeline Performance) - Show architecture & performance
- Reference **Figure 1** to justify model choice

### Results Section
- **Figure 1** (Model Comparison) - Lead with this
- **Figure 2** (Modality Distribution) - Show extraction patterns
- **Figure 3** (Severity Analysis) - Demonstrate conflict classification
- **Figure 4** (Temporal Accuracy) - Validate temporal component
- **Figure 5** (Canon Application) - Show resolution effectiveness
- **Figure 6** (Corpus Comparison) - Justify corpus selection

### Discussion Section
- **Figure 8** (Cost-Quality) - Discuss practical deployment considerations

### Supplementary Material
- All figures with extended captions
- `generate_figures.py` script for reproducibility

---

## Reproducibility

All figures can be regenerated:
```bash
python generate_figures.py
```

Data sources:
- `outputs/*/json/` - Extraction results
- `docs/RESULTS.md` - Aggregated metrics
- Manual validation data

---

## File Locations

```
paper/
‚îú‚îÄ‚îÄ figures/
‚îÇ   ‚îú‚îÄ‚îÄ figure1_model_comparison.png       (300 DPI)
‚îÇ   ‚îú‚îÄ‚îÄ figure2_modality_distribution.png  (300 DPI)
‚îÇ   ‚îú‚îÄ‚îÄ figure3_severity_analysis.png      (300 DPI)
‚îÇ   ‚îú‚îÄ‚îÄ figure4_temporal_accuracy.png      (300 DPI)
‚îÇ   ‚îú‚îÄ‚îÄ figure5_canon_application.png      (300 DPI)
‚îÇ   ‚îú‚îÄ‚îÄ figure6_corpus_comparison.png      (300 DPI)
‚îÇ   ‚îú‚îÄ‚îÄ figure7_pipeline_performance.png   (300 DPI)
‚îÇ   ‚îú‚îÄ‚îÄ figure8_cost_quality_tradeoff.png  (300 DPI)
‚îÇ   ‚îî‚îÄ‚îÄ README.md                           (Figure descriptions)
‚îî‚îÄ‚îÄ FIGURES_SUMMARY.md                      (This file)
```

---

## Statistics Summary

**Total Figures**: 8
**Total File Size**: ~2 MB
**Resolution**: 300 DPI (publication quality)
**Format**: PNG (high compatibility)
**Generation Time**: <10 seconds

**Visualizations Created**:
- 12 bar charts
- 4 line plots
- 2 scatter plots
- 1 pie chart
- 1 radar chart
- 1 timeline
- Multiple combined visualizations

---

## Next Steps

1. ‚úÖ Review all figures in `paper/figures/`
2. ‚úÖ Read detailed captions in `figures/README.md`
3. ‚è≠Ô∏è Integrate into paper LaTeX/Word document
4. ‚è≠Ô∏è Cite figure source code for reproducibility
5. ‚è≠Ô∏è Consider additional ablation studies if reviewers request

---

**Status**: ‚úÖ Complete and ready for paper submission

